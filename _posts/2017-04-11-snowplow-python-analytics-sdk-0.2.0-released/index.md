---
layout: post
title: Snowplow Python Analytics SDK 0.2.0 released
title-short: Snowplow Python Analytics SDK 0.2.0
tags: [python, snowplow, enriched events, spark, dynamodb]
author: Anton
category: Releases
permalink: /blog/2017/04/11/snowplow-python-analytics-sdk-0.2.0-released/
---

We are pleased to announce the 0.2.0 release of the [Snowplow Python Analytics SDK][sdk-repo], a library providing tools to process and analyze Snowplow enriched event format in Python-compatible data processing frameworks such as [Apache Spark][spark] and [AWS Lambda][lambda].

This release adds new run manifest functionality, along with many internal changes.

In the rest of this post we will cover:

1. [Run manifests](#run-manifests)
2. [Using the run manifest](#using-manifests)
3. [Documentation](#documentation)
4. [Other changes](#other)
5. [Upgrading](#upgrading)
6. [Getting help](#help)

<!--more-->

<h2 id="run-manifests">1. Run manifests</h2>

This release provides tooling for maintaining a **Snowplow run manifest**. A Snowplow run manifest is a simple and robust way to track your job's progress in processing the enriched events generated by multiple Snowplow pipeline runs.

Historically, Snowplow's EmrEtlRunner and StorageLoader apps have moved whole folders of data around different locations in Amazon S3 in order to track progress through a pipeline run, and to avoid accidentally reprocessing that data. But file moves are quite problematic:

1. They are time-consuming
2. They are network-intensive
3. They are error-prone - a failure to move a file will cause the job to fail and require manual intervention
4. They only support one use-case at a time - you can't have two distinct jobs moving the same files at the same time

Although Snowplow continues to use file moves (for now), it is better to use a run manifest for your own data processing jobs on Snowplow data. The idea of a manifest comes from the old naval term:

> a list of the cargo carried by a ship, made for the use of various agents and officials at the ports of destination

In this case, we store our manifest in a [AWS DynamoDB][dynamodb] table, and we use it to keep track of which Snowplow runs our job has already processed.

<h2 id="using-the-manifest">2. Using the run manifest</h2>

The run manifest functionality resides in the new `snowplow_analytics_sdk.run_manifests` module.

Here's a short usage example:

{% highlight python %}
from boto3 import client
from snowplow_analytics_sdk.run_manifests import *

s3 = client('s3')
dynamodb = client('dynamodb')

dynamodb_run_manifests_table = 'snowplow-run-manifests'
enriched_events_archive = 's3://acme-snowplow-data/storage/enriched-archive/'
run_manifests = RunManifests(dynamodb, dynamodb_run_manifests_table)

run_manifests.create() # This should be called only once

for run_id in list_runids(s3, enriched_events_archive):
    if not run_manifests.contains(run_id):
        process(run_id)
        run_manifests.add(run_id)
    else:
        pass
{% endhighlight %}

In above example, we create two AWS service clients, one for S3 (to list job runs) and for DynamoDB (to access our manifest). These clients are provided via [boto3][boto3] Python AWS SDK and can be initialized with static credentials or with system-provided credentials.

Then we list all Snowplow runs in a particular S3 path, and then process (with the user-provided `process` function) **only** those Snowplow runs that had not been previously processed. Note that `run_id` is just a simple string with the S3 key of particular job run.

`RunManifests` class, then, is a simple API wrapper to DynamoDB, which lets you:

* `create` a DynamoDB table for manifests
* `add` a Snowplow run to the table
* check if table `contains` a given run ID

<h2 id="documentation">3. Documentation</h2>

As an SDK becomes more featureful it becomes harder to keep all the required documentation in the project's README.
In this release we have split out the README into several wiki pages, each dedicated to a particular feature.

Check out the [Python Analytics SDK][sdk-docs] in the main Snowplow wiki.

<h2 id="other">4. Other changes</h2>

Version 0.2.0 also includes a few internal changes and minor enhancements, including:

* Adding a Vagrant environment ([issue #5][issue-5])
* Support for multiple versions of Python ([issue #16][issue-16])
* Strict PEP8 linting ([issue #4][issue-4]) for the CI tests

<h2 id="upgrading">5. Upgrading</h2>

As before, the Snowplow Python Analytics SDK is available on [PyPI][pypi]:

{% highlight python %}
pip install -U snowplow_analytics_sdk==0.2.0
{% endhighlight %}

<h2 id="help">6. Getting help</h2>

If you have any questions or run into any problems, please [raise an issue][issues] or get in touch with us through [the usual channels][talk-to-us].

And if there's another Snowplow Analytics SDK you'd like us to prioritize creating, please let us know on the [forums] [discourse]!

[sdk-repo]: https://github.com/snowplow/snowplow-python-analytics-sdk
[sdk-usage-img]: /assets/img/blog/2016/03/scala-analytics-sdk-usage.png
[sdk-docs]: https://github.com/snowplow/snowplow/wiki/Python-Analytics-SDK

[boto3]: https://boto3.readthedocs.io/en/latest/
[dynamodb]: https://aws.amazon.com/dynamodb/

[event-data-modeling]: /blog/2016/03/16/introduction-to-event-data-modeling/

[spark]: http://spark.apache.org/
[lambda]: https://aws.amazon.com/lambda/

[issues]: https://github.com/snowplow/snowplow-python-analytics-sdk/issues
[issue-4]: https://github.com/snowplow/snowplow-python-analytics-sdk/issues/4
[issue-5]: https://github.com/snowplow/snowplow-python-analytics-sdk/issues/5
[issue-16]: https://github.com/snowplow/snowplow-python-analytics-sdk/issues/16

[pypi]: https://pypi.python.org/pypi/snowplow_analytics_sdk

[talk-to-us]: https://github.com/snowplow/snowplow/wiki/Talk-to-us
[discourse]: http://discourse.snowplowanalytics.com/
